---
title: "学習の仕組み"
tags: [学習, 基礎, モデル, 評価]
created: 2026-02-25
updated: 2026-02-26
---

## モデルはどうやって学ぶのか

モデルの学習は、予測と正解の誤差を[[損失関数]]で数値化し、その値を小さくする方向に[[パラメータ]]を更新する繰り返しです。更新には[[勾配降下法]]が使われ、損失関数の勾配（微分値）を計算してパラメータを調整します。一度に処理するデータ量を[[バッチサイズ]]、更新の幅を[[学習率]]と呼びます。訓練データ全体を1周する単位が[[エポック]]で、通常は数十〜数百エポックの学習を行います。

## 学習の先にあるもの

学習が完了したモデルを使って新しいデータの予測を行うことを[[推論（Inference）]]と呼びます。ただし、学習しすぎると訓練データだけに過度に適合する[[過学習（Overfitting）]]が起こります。モデルの予測精度は[[Accuracy]]などの指標で評価します。また、大規模モデルを特定のタスクに適応させる[[ファインチューニング]]も広く使われており、少量のデータで高精度な専用モデルを作れます。

## 用語

- **Loss Function（損失関数）**: モデルの予測と正解との誤差を数値化する関数。交差エントロピー損失、MSE等がある。この値を最小化するようにパラメータが更新される。
- **Gradient Descent（勾配降下法）**: 損失関数の勾配（微分）を計算し、パラメータを勾配の逆方向に更新する最適化手法。SGD、Adam、AdaGrad等の変種がある。
- **Batch Size（バッチサイズ）**: 1回のパラメータ更新で使用するデータサンプル数。大きいほど安定した勾配推定が得られるが、メモリ使用量が増加する。
- **Learning Rate（学習率）**: パラメータ更新の幅を制御するハイパーパラメータ。大きすぎると発散し、小さすぎると収束が遅くなる。AdamやSGD等のオプティマイザと組み合わせて使用。
- **Epoch（エポック）**: 訓練データ全体を1回通して学習する単位。複数エポックの学習でモデルの精度が向上するが、過学習のリスクもある。
- **Inference（推論）**: 学習済みモデルを使って新しい入力に対する予測を行うこと。学習（訓練）とは異なり、パラメータの更新は行わない。レイテンシやスループットが重要な指標となる。
- **Overfitting（過学習）**: 訓練データに過度に適合し、未知のデータに対する汎化性能が低下する現象。ドロップアウト、正則化、データ拡張等で対策する。
- **Accuracy（精度）**: 全予測のうち正解した割合。分類タスクの基本的な評価指標だが、クラス不均衡時には不適切な場合がある。Precision、Recall、F1スコア等と併用する。
- **Fine-tuning（ファインチューニング）**: 事前学習済みモデルを特定のタスク向けに追加学習すること。全パラメータを更新するFull Fine-tuningと、一部のみ更新するLoRA等のパラメータ効率的手法がある。
