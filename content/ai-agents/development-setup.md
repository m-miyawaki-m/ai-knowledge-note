---
title: "開発準備"
tags: [基礎, 設計, 構成要素, 安全性, ツール, フレームワーク, 開発, API]
created: 2026-02-25
updated: 2026-02-26
---

## Chat Completions APIとモデルの基本

エージェント開発の出発点は[[Chat Completions API]]です。system（システム指示）、user（ユーザー入力）、assistant（AI応答）のロールでメッセージを送り、AIの応答を得ます。代表的なモデルとして、GPT-4o（高性能・マルチモーダル）、GPT-4o-mini（高速・低コスト）などがあり、タスクの要件に応じて選択します。[[推論モデル]]（o1、o3等）は内部で段階的な推論を行い、複雑な論理的思考が可能です。エージェントの計画フェーズでの活用が期待されますが、応答速度が遅くコストも高いため、計画など高度な推論が必要な場面に限定して使うのが効果的です。

## 構造化出力とPrompt Caching

[[構造化出力]]は、LLMの出力をJSONスキーマに従った形式で強制する機能です。エージェントではツール呼び出しの引数や計画の出力で必須の機能で、パース失敗のリスクを排除します。[[プロンプトキャッシング]]は、同一のプロンプトプレフィックス（システムプロンプトや大きなコンテキスト）をキャッシュし、API呼び出しのコストと遅延を削減します。エージェントは同じシステムプロンプトで何度もAPIを呼ぶため、この最適化の効果が大きくなります。

## Function Callingとエージェント用ツール

[[Function Calling]]はLLMが外部関数を呼び出す機能で、エージェントのツール利用の基盤です。APIリクエストにツールの定義（名前・説明・引数スキーマ）を含めると、LLMは適切なタイミングでツール呼び出しをJSON形式で返します。エージェントでよく使われるツールとして、(1)Web検索：最新情報の取得、(2)RAG（非公開情報の検索）：社内ドキュメントやマニュアルの検索に[[Embedding API]]でベクトル化した情報を使う、(3)[[Code Interpreter]]：生成したコードの実行。[[Assistants API]]はこれらを統合的に提供するOpenAIのAPIです。

## LangGraphによるワークフロー構築

[[LangGraph]]はグラフベースのエージェントワークフロー構築フレームワークです。ステート（状態）を定義し、ノード（処理）とエッジ（遷移）でワークフローを構築します。条件分岐（conditional edge）により、LLMの判断結果に応じてフローを動的に切り替えられます。基本的な構築手順は、(1)State型の定義、(2)各ノード関数の実装、(3)グラフの構築（ノード追加→エッジ定義）、(4)コンパイル→実行。LangGraphの利点は、複雑なループや分岐を含むワークフローを宣言的に定義できること、状態のスナップショットによるデバッグのしやすさ、[[ヒューマンインザループ]]の組み込みが容易なことです。

## ローカルLLMとAPIの選択肢

OpenAI以外にも、Anthropic（Claude）、Google（Gemini）、Mistral等のAPIが利用可能です。[[ローカルLLM]]はOllama等を使いローカル環境で動作させる選択肢で、API費用がかからず、データがローカルに留まるため機密性が高いのが利点です。ただし、モデルサイズが大きいほど高性能なGPUが必要で、クラウドAPIのモデルと比較すると性能面で劣る場合があります。エージェント開発では、計画や複雑な推論にはクラウドの高性能モデル、繰り返しの簡単なタスクにはローカルLLMという使い分けも有効です。

## 用語

- **Chat Completions API（チャット補完API）**: OpenAI等が提供する、会話形式でLLMを呼び出すAPI。system/user/assistantのロールでメッセージを送り、AIの応答を得る。エージェント開発の基盤となるインターフェース。
- **Reasoning Models（推論モデル）**: o1、o3等の、内部で段階的な推論プロセスを実行するLLM。通常のモデルより複雑な論理的思考が可能で、計画立案や数学的推論に優れる。エージェントの計画フェーズでの活用が期待される。
- **Prompt Caching（プロンプトキャッシング）**: 同一のプロンプトプレフィックスを再利用し、API呼び出しのコストと遅延を削減する技術。長いシステムプロンプトや大量のコンテキストを使うエージェントで特に有効。
- **Embedding API（エンベディングAPI）**: テキストをベクトル（数値の配列）に変換するAPI。意味的に近いテキスト同士が近いベクトルになる性質を利用し、セマンティック検索やRAGの基盤技術として使われる。
- **Code Interpreter（コードインタープリタ）**: AIが生成したプログラムコードをサンドボックス環境で実行し、結果を取得する仕組み。データ分析、数値計算、ファイル変換など、LLMの計算能力を補完するツールとして使われる。
- **Assistants API（アシスタントAPI）**: OpenAIが提供する、ステートフルなAIアシスタントを構築するためのAPI。スレッド管理、ファイル処理、Code Interpreter、Function Callingを統合的に提供する。
- **Local LLM（ローカルLLM）**: Ollama等を使いローカル環境で動作させるLLM。API費用がかからず、データがローカルに留まるため機密性が高い。ただし、モデルサイズやGPUリソースによる性能の制約がある。
